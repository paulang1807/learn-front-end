{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $$Statistics \\ Cheat \\ Sheet \\ - \\ Regression$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares Regression\n",
    "**Simple Linear Regression Model**\n",
    "\n",
    "Also called *two-variable linear regression model* or *bivariate linear regression model*\n",
    "$$y=\\beta_0 + \\beta_1 x + u$$\n",
    "where $y$ is the dependent variable (also called explained,outcome,predicted or response variable or just regressand),\n",
    "$\\beta_0$ is the intercept,\n",
    "$\\beta_1$ is the slope parameter,\n",
    "$x$ is the independent variable (also called control, predictor or explanatory variable or just regressor or covariate) and\n",
    "$u$ is the error term or disturbance and includes the effect of all other factors aside from $x$\n",
    "\n",
    "* $\\beta_0 + \\beta_1 x$ is referred to as the **systematic part** of $y$, the part that can be expalined by $x$ and $u$ is the **unsystematic part** of $y$, the part that cannot be expalined by $x$\n",
    "\n",
    "If all other factors (disturbance) are fixed, i.e. if $\\Delta u=0$, then\n",
    "$$\\Delta y = \\beta_1 \\Delta x$$\n",
    "\n",
    "**Error Assumptions**\n",
    "* Errors have zero means $E(u) = 0$\n",
    "* Average value of $u$ does not depend on value of $x$, i.e. $u$ is **mean independent** of x, $E(u|x)=E(u) \\implies E(u|x) = 0$. This is referred to as the **zero conditional mean assumption**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Population Regression Function (PRF)**\n",
    "$$E(y|x) = E(\\beta_0 + \\beta_1x + u|x) = \\beta_0 + \\beta_1x$$\n",
    "PRF is something fixed, but unknown, in the population. \n",
    "\n",
    "**Regression Residual**\n",
    "$$\\hat u = y_i - \\hat y_i = y_i - \\hat \\beta_0 - \\hat \\beta_1x_i$$\n",
    "where $\\hat u$, $\\hat y_i$,$\\hat \\beta_0$ and $\\hat \\beta_1$ are the estimated values. Here $\\hat u$ is different from the error term $u$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OLS Estimates**\n",
    "$$\\hat \\beta_1 = \\frac{\\sum_{i=1}^n(x_i - \\overline x)(y_i - \\overline y)}{\\sum_{i=1}^n(x_i - \\overline x)^2} = \\frac{\\hat{cov}(x_i, y_i)}{\\hat{var}(x_i)}$$\n",
    "$$=\\hat \\rho_{xy} \\cdot \\biggl(\\frac{\\hat \\sigma_x}{\\hat \\sigma_y}\\biggl)$$\n",
    "$$\\hat \\beta_0 = \\overline y - \\hat \\beta_1 \\overline x$$\n",
    "\n",
    "The **fitted or predicted value** for $y$ when $x=x_i$ is $\\hat y_i = \\hat \\beta_0 + \\hat \\beta_1 x_i$ and defines the ith point on the **OLS Regression Line**\n",
    "\n",
    "Also, $\\hat \\beta_1 = \\frac{S_{xy}}{S_{xx}}$\n",
    "where $S_{xy} = \\sum xy - \\frac{\\sum x \\sum y}{n}$\n",
    "and $S_{xx} = \\sum x^2 - \\frac{(\\sum x)^2}{n}$\n",
    "\n",
    "**Properties**\n",
    "* Estimated errors sum up to zero, $\\sum_{i=1}^n \\hat u_i = 0$\n",
    "* Correlation between residuals and regressors is zero, $\\sum_{i=1}^n x_i \\hat u_i = 0$\n",
    "* Sample averages of y and x (point $\\overline x, \\overline y$) lie on a regression line, $\\overline y = \\hat \\beta_0 + \\hat \\beta_1 \\overline x$\n",
    "\n",
    "**Sum of Squared Residuals**\n",
    "$$\\sum_{i=1}^n \\hat u_i^2 = \\sum_{i=1}^n (y_i - \\hat y_i)^2= \\sum_{i=1}^n (y_i - \\hat \\beta_0 - \\hat \\beta_1 x_i)^2$$\n",
    "\n",
    "**First Order Conditions for OLS estimates**\n",
    "$$n^{-1} \\sum_{i=1}^n (y_i - \\hat \\beta_0 - \\hat \\beta_1 x_i) = 0$$\n",
    "$$n^{-1} \\sum_{i=1}^n x_i(y_i - \\hat \\beta_0 - \\hat \\beta_1 x_i) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample Regression Function (SRF) or OLS Regression Line**\n",
    "$$\\hat y = \\hat \\beta_0 + \\hat \\beta_1 x$$\n",
    "\n",
    "**Slope Estimate**\n",
    "$$\\hat \\beta_1 = \\Delta \\hat y / \\Delta x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Measures of Variation**\n",
    "* Total Sum of Squares: $SST = \\sum_{i=1}^n(y_i - \\overline y)^2$\n",
    "* Explained Sum of Squares: $SSE = \\sum_{i=1}^n(\\hat y_i - \\overline y)^2$\n",
    "* Residual Sum of Squares: $SSR = \\sum_{i=1}^n \\hat u_i^2$\n",
    "$$SST = SSE + SSR$$\n",
    "* Mean Square of Regression, $MSR = SSR/degrees \\ of \\ freedom \\ of \\ SSR$\n",
    "\n",
    "**Goodness of fit measure**\n",
    "$$R^2 = \\frac{SSE}{SST} = 1 - \\frac{SSR}{SST}$$\n",
    "is the fraction of the total variation explained by regression. This holds assuming that the total sum of squares, $SST$, is not equal to zeroâ€”which is true except in the very unlikely event that all the $y_i$ equal the same value. $R^2$ is also called **coefficient of determination**\n",
    "\n",
    "* The value of $R^2$ is always between zero and one, because SSE can be no greater than SST.\n",
    "\n",
    "$$R^2 = cor(Y, x)^2$$ \n",
    "$$R^2 = var(model$fitted)/var(Y)$$  \n",
    "$$R^2 = (coef(model)[2]/ \\sqrt{cov(Y, Y)/cov(x, x)})^2 = (\\beta_1/\\sqrt{cov(Y, Y)/cov(x, x)})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semilogarithmic Form**\n",
    "$$log(y) = \\beta_0 + \\beta_1x + u$$\n",
    "$$\\beta_1 = \\frac{\\delta y/y}{\\delta x}$$\n",
    "\n",
    "$$\\hat {log}(y) = \\hat \\beta_0 + \\hat \\beta_1x$$\n",
    "\n",
    "**Interpretation of Slope**\n",
    "$$\\Delta y = \\beta_1 \\Delta x \\ \\ \\text{, for level-level (y,x) model}$$\n",
    "$$\\Delta y = (\\beta_1/100) \\text{%} \\Delta x \\ \\ \\text{, for level-log (y, log(x)) model}$$\n",
    "$$\\text{%} \\Delta y = (100\\beta_1) \\Delta x \\ \\ \\text{, for log-level (log(y),x) model}$$\n",
    "$$\\text{%} \\Delta y = \\beta_1 \\text{%} \\Delta x \\ \\ \\text{, for log-log (log(y), log(x)) model}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unbiasedness of OLS**\n",
    "$$E(\\hat \\beta_0) = \\beta_0; E(\\hat \\beta_1) = \\beta_1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Homoskedasticity**\n",
    "$$Var(u|x) = \\sigma^2$$ and\n",
    "$$Var(y|x) = \\sigma^2$$\n",
    "\n",
    "**Error Variance**\n",
    "$$\\sigma^2 = E(u^2) = Var(u)$$\n",
    "\n",
    "* $n^{-1}\\sum_{i-1}^nu_i^2$ is an unbiased estimator of $\\sigma^2$. This is however not a true estimator.\n",
    "* $n^{-1}\\sum_{i-1}^n \\hat u_i^2 = SSR/n$ is a true estimator of $\\sigma^2$ (though it is biased, the bias being very small for large n). \n",
    "* $\\hat \\sigma^2 = \\frac{1}{n - 2}\\sum_{i=1}^n \\hat u_i^2 = SSR/(n - 2)$ makes use of the degrees of freedom adjustment and is an unbiased estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sampling Variances of the OLS Estimators**\n",
    "$$Var(\\hat \\beta_1) = \\frac{\\sigma^2}{SST_x}$$\n",
    "$$Var(\\hat \\beta_0) = \\frac{\\sigma^2n \\sum_{i=1}^nX_i^2}{\\sum_{i=1}^n(X_i - \\overline X)^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Errors and Residuals**\n",
    "$$\\hat u_i = u_i - (\\hat \\beta_0 - \\beta_0) - (\\hat \\beta_1 - \\beta_1)x_i$$\n",
    "\n",
    "**Standard Error of Regression (SER)**\n",
    "$$\\hat \\sigma = \\sqrt{\\hat \\sigma^2}$$\n",
    "This is also called *standard error of the estimate* or *the root mean square error* and is the natural estimator of $\\sigma$\n",
    "\n",
    "**Standard Error of $\\hat \\beta_1$**\n",
    "$$se(\\hat \\beta_1) = \\frac{\\hat \\sigma}{\\sqrt{SST_x}} = \\hat \\sigma / \\biggl( \\sum_{i=1}^n(x_i - \\overline x)^2 \\biggl)^{1/2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression Through The Origin**\n",
    "$$\\tilde y = \\tilde \\beta_1x$$\n",
    "where $\\tilde \\beta_1$ is the OLS estimate from the regression of $y$ on $x$ through the origin\n",
    "$$\\tilde \\beta_1 = \\frac{\\sum_{i=1}^nx_iy_i}{\\sum_{i=1}^nx_i^2}$$\n",
    "This is equal to the slope estimate $\\beta_1 \\ iff\\  \\overline x = 0$\n",
    "\n",
    "* If the intercept $\\beta_0 \\neq 0$, then $\\tilde \\beta_1$ is a biased estimator of $\\beta_1$.\n",
    "\n",
    "$$R^2 = 1 - \\frac{\\sum_{i=1}^n(y_i - \\tilde \\beta_1x_i)^2}{\\sum_{i=1}^n(y_i - \\overline y)^2}$$\n",
    "A more common version is\n",
    "$$R^2 = 1 - \\frac{\\sum_{i=1}^n(y_i - \\tilde \\beta_1x_i)^2}{\\sum_{i=1}^ny_i^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consistency of OLS**\n",
    "\n",
    "Probability Limit $plim_{n \\to \\infty} \\hat \\beta_j = \\beta_j$, ,i.e. coefficients equal the true parameters as n approaches infinity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Regression or Multiple Linear Regression (MLR)\n",
    "$$y=\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k + u$$\n",
    "where $\\beta_1, \\beta_2,..., \\beta_k$ are referred to as the **slope parameters**\n",
    "$$x_1 = \\delta_0 + \\delta_2 x_2 + ... + \\delta_k x_k + r_1$$\n",
    "$$y = \\gamma_0 + \\gamma_1 r_1 + \\nu$$\n",
    "\n",
    "**Key Assumption**\n",
    "$$E(u | x_1, x_2,...,x_k) = 0$$\n",
    "i.e. all factors in the unobserved error term are uncorrelated with the explanatory variables.\n",
    "\n",
    "**Partial Effects**\n",
    "\n",
    "Change in $y$ for given changes in $x_1, x_2,...,x_k$ is\n",
    "$$\\Delta y = \\beta_1 \\Delta x_1 + \\beta_2 \\Delta x_2 + ... + \\beta_k \\Delta x_k$$\n",
    "If $x_2,...,x_k$ are fixed then\n",
    "$$\\beta_1 = \\Delta y / \\Delta x_1$$\n",
    "is called the partial effect of $x_1$ on y.\n",
    "\n",
    "**Sum of Squared Residuals**\n",
    "$$\\sum_{i=1}^n \\hat u_i^2 = \\sum_{i=1}^n (y_i - \\hat \\beta_0 - \\hat \\beta_1 x_{i1} - ... - \\hat \\beta_k x_{ik})^2$$\n",
    "where the subscript $i$ represents the observation number and $k$ represents the independent variable.\n",
    "\n",
    "**First Order Conditions for OLS estimates**\n",
    "$$n^{-1} \\sum_{i=1}^n (y_i - \\hat \\beta_0 - \\hat \\beta_1 x_{i1} - ... - \\hat \\beta_k x_{ik}) = 0$$\n",
    "$$n^{-1} \\sum_{i=1}^n x_{i1}(y_i - \\hat \\beta_0 - \\hat \\beta_1 x_{i1} - ... - \\hat \\beta_k x_{ik}) = 0$$\n",
    "$$n^{-1} \\sum_{i=1}^n x_{i2}(y_i - \\hat \\beta_0 - \\hat \\beta_1 x_{i1} - ... - \\hat \\beta_k x_{ik}) = 0$$\n",
    "$$\\vdots$$\n",
    "$$n^{-1} \\sum_{i=1}^n x_{ik}(y_i - \\hat \\beta_0 - \\hat \\beta_1 x_{i1} - ... - \\hat \\beta_k x_{ik}) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Slope Estimates**\n",
    "\n",
    "One way to express the slope estimate is\n",
    "$$\\hat \\beta_1 = \\frac{\\sum_{i=1}^n \\hat r_{i1}y_i}{\\sum_{i=1}^n \\hat r_{i1}^2}$$\n",
    "where $\\hat r_{i1}$ are the OLS residuals from a simple regression of $x_1$ on $x_2,...,x_k$\n",
    "$$x_1 = \\alpha_0 + \\alpha_1 x_2 + r_{i1}$$\n",
    "\n",
    "**Regression Anatomy Formula**\n",
    "$$\\beta_1 = \\frac{cov(r_1, y)}{var(r_1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression Formula in Matrix Form**\n",
    "$$y = X \\beta + u$$\n",
    "where $y$ is a $n \\times 1$ vector, $X$ is a $n \\times (k+1)$ matrix, $\\beta$ is a $(k + 1) \\times 1$ vector and $u$ is a $n \\times 1$ vector\n",
    "\n",
    "**OLS Estimator in Matrix Form**\n",
    "$$\\beta_1 = (X'X)^{-1}X'Y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardized Residuals - Rough Method**\n",
    "$$r_i = \\frac{e_i}{s \\sqrt{1 - h_{ii}}}$$\n",
    "where $e_i$ is the residual, $s$ is the standard error and $h_{ii}$ is the leverage\n",
    "* Values >3 are very large, not likely due to chance.\n",
    "* If 1% (or more) of cases have residuals >2.5, model contains too much error.\n",
    "* If 5% (or more) of cases have residuals >2, model has too much error and represents our data poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Studentized Residuals - Precise Method**\n",
    "$$t_i = \\frac{e_i}{s_{-i} \\sqrt{1 - h_i}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Omitted Variable Bias for Simple Regression**\n",
    "\n",
    "If the true (population) model is \n",
    "$$Y=\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + u$$\n",
    "and the estimated model is \n",
    "$$Y=\\alpha_0 + \\alpha_1 x_1 + v$$\n",
    "where \n",
    "$$x_2 = \\delta_0 + \\delta_1 x_1 + w$$\n",
    "then\n",
    "$$\\alpha_1 = \\beta_1 + \\beta_2 \\delta_1$$\n",
    "where $\\beta_2 \\delta_1$ is the Omitted Variable Bias for Simple Regression\n",
    "$$Bias(\\alpha_1) = E(\\alpha_1) - \\beta_1 = \\beta_2 \\delta_1$$ and\n",
    "$$\\delta_1 = Cov(x_1, x_2)/Var(x_1)$$ is the **inconsistency**\n",
    "\n",
    "The **signs** of $\\beta_2$ and $\\delta_1$ will estimate the direction of bias on $x_1$\n",
    "\n",
    "The **size** of the bias is determined by the sizes of $\\beta_2$ and $\\delta_1$.\n",
    "\n",
    "The slope coefficient of $x_1$ for the true model $(\\beta_1)$ and estimated model $(\\alpha_1)$ will be equal (and hence $\\alpha_1$ will be unbiased) if\n",
    "- The partial effect of $x_2$ on $Y$ is 0, i.e. $\\beta_2 = 0$\n",
    "- $x_1$ and $x_2$ are uncorrelated, i.e. $\\delta_1 = 0$\n",
    "\n",
    "**Bias Matrix**\n",
    "- If $\\beta_2 \\gt 0$ and $Corr(x_1, x_2) \\gt 0$, we have positive bias\n",
    "- If $\\beta_2 \\gt 0$ and $Corr(x_1, x_2) \\lt 0$, we have negative bias\n",
    "- If $\\beta_2 \\lt 0$ and $Corr(x_1, x_2) \\gt 0$, we have negative bias\n",
    "- If $\\beta_2 \\lt 0$ and $Corr(x_1, x_2) \\lt 0$, we have positive bias\n",
    "\n",
    "- If $E(\\alpha_1) \\gt \\beta_1$, then we say that $\\alpha_1$ has an **upward bias**\n",
    "- If $E(\\alpha_1) \\lt \\beta_1$, then we say that $\\alpha_1$ has an **downward bias**\n",
    "- If $E(\\alpha_1)$ is closer to zero than is $\\beta_1$, then we say that $\\alpha_1$ is **biased toward zero**\n",
    "- If $\\beta_1$ is positive, then $\\alpha_1$ is biased toward zero if it has a downward bias. \n",
    "- If $\\beta_1$ is negative, then $\\alpha_1$ is biased toward zero if it has a upward bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the true (population) model is \n",
    "$$Y=\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + u$$\n",
    "and the estimated model is \n",
    "$$Y=\\alpha_0 + \\alpha_1 x_1 + \\alpha_2 x_2 + v$$\n",
    "and $x_1$ and $x_3$ are correlated but $x_1$ and $x_2$ are uncorrelated, then the bias in $\\alpha_1$\n",
    "$$E(\\alpha_1) = \\beta_1 + \\beta_3 \\frac{\\sum_{i=1}^n(x_{i1} - \\overline x_1)x_{i3}}{\\sum_{i=1}^n(x_{i1} - \\overline x_1)^2}$$\n",
    "\n",
    "**Bias Matrix**\n",
    "- If $\\beta_3 \\gt 0$ and $Corr(x_1, x_3) \\gt 0$, we have positive bias\n",
    "- If $\\beta_3 \\gt 0$ and $Corr(x_1, x_3) \\lt 0$, we have negative bias\n",
    "- If $\\beta_3 \\lt 0$ and $Corr(x_1, x_3) \\gt 0$, we have negative bias\n",
    "- If $\\beta_3 \\lt 0$ and $Corr(x_1, x_3) \\lt 0$, we have positive bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Properties of Fitted Values and Residuals**\n",
    "- The sample average of the residuals is zero and so $\\overline y = \\overline {\\hat y}$ \n",
    "- The sample covariance between each independent variable and the OLS residuals is zero. Consequently, the sample covariance between the OLS fitted values and the OLS residuals is zero. \n",
    "$$\\sum_{i=1}^n x_{ij} \\hat u_i = 0$$\n",
    "- The point $(\\overline x_1, \\overline x_2, ... , \\overline x_k, \\overline y)$ is always on the OLS regression line:\n",
    "$$\\overline y=\\hat \\beta_0 + \\hat \\beta_1 \\overline x_1 + \\hat \\beta_2 \\overline x_2 + ... + \\hat \\beta_k \\overline x_k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation Coefficient**\n",
    "$$R^2 = \\frac{\\biggl(\\sum_{i=1}^n(y_i - \\overline y)(\\hat y_i - \\overline {\\hat y})\\biggl)^2}{\\biggl(\\sum_{i=1}^n(y_i - \\overline y)^2\\biggl)\\biggl(\\sum_{i=1}^n(\\hat y_i - \\overline {\\hat y})^2\\biggl)}$$\n",
    "\n",
    "**Asymptotic Bias**\n",
    "$$plim \\ \\tilde \\beta_1 - \\beta_1 = Cov(x_1, u)/Var(x_1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Durban Watson Statistic**\n",
    "$$d=\\frac{\\sum_{t=2}^T(e_t - e_{t-1})^2}{\\sum_{t=2}^T e_t^2}$$\n",
    "This statistic compares the differences between successive data points to the magnitude of the data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sampling Variance of OLS Estimators**\n",
    "$$Var(\\hat \\beta_j) = \\frac{\\sigma^2}{SST_j(1 - R_j^2)}, j=1,2,...,k$$\n",
    "where $SST_j = \\sum_{i-1}^n (x_{ij} - \\overline x)^2$ is the total sample variation in $x_j$ , and $R_j^2$ is the R-squared from regressing $x_j$ on all other independent variables (and including an intercept).\n",
    "\n",
    "**Variance Inflation Factor**\n",
    "$$VIF_j = \\frac{1}{(1 - R_j^2)}$$\n",
    "$$\\implies Var(\\hat \\beta_j) = \\frac{\\sigma^2}{SST_j} \\cdot VIF_j$$\n",
    "where $1 - R^2$ is the **Tolerance**\n",
    "\n",
    "VIF of 10 is regarded as very high. Tolerance values of 0.10 or less ($R_j^2$ values of .9 or more)Indicate that there may be serious multicollinearity. ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unbiased Estimator of Variance**\n",
    "$$\\hat \\sigma^2 = \\frac{\\biggl(\\sum_{i=1}^n \\hat u_i^2\\biggl)}{n - k - 1} = \\frac{SSR}{n - k - 1}$$\n",
    "where $$\\hat u_i = y_i - \\hat \\beta_0 - \\hat \\beta_1x_{i1} - \\hat \\beta_2x_{i2} - ... - \\hat \\beta_kx_{ik}$$\n",
    "and $$n - k - 1 = n - (k + 1) = \\text{(number of observations)} - \\text{(number of estimated parameters)}$$ is the degree of freedom\n",
    "\n",
    "Also, expected value of the sum of squared residuals is $E(SSR) = (n - k - 1) \\sigma^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standard Deviation of $\\hat \\beta_j$**\n",
    "$$sd(\\hat \\beta_j) = \\frac{\\sigma}{\\sqrt{SST_j(1 - R_j^2)}}$$\n",
    "\n",
    "**Standard Error of $\\hat \\beta_j$**\n",
    "$$se(\\hat \\beta_j) = \\frac{\\hat \\sigma}{\\sqrt{SST_j(1 - R_j^2)}}$$\n",
    "\n",
    "In case of heteroskedasticity,\n",
    "$$sd(\\hat \\beta_j) = \\frac{\\hat \\sigma}{\\sqrt n sd(x_j) \\sqrt{1 - R_j^2}}$$\n",
    "where $$sd(x_j) = \\sqrt{n^{-1}\\sum_{i=1}^n(x_{ij} - \\overline x_j)^2}$$ is the sample standard deviation\n",
    "\n",
    "Under **OLS Asymptotics**\n",
    "$$se(\\hat \\beta_j) \\approx c_j / \\sqrt n$$\n",
    "where $c_j$ is a positive constatnt that doesn't depend on the sample size and\n",
    "$$c_j = \\frac{\\sigma}{\\sigma_j \\sqrt{1 - \\rho_j^2}}$$\n",
    "where $\\sigma = sd(u), \\sigma_j = sd(x_j)$ and $\\rho_j^2$ is the population $R^2$ from regressing $x_j$ on the other explanatory variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normality Assumptions**\n",
    "$$\\hat \\beta_j \\sim N(\\beta_j, Var(\\hat \\beta_j))$$\n",
    "Normalizing the estimator gives\n",
    "$$\\frac {\\hat \\beta_j - \\beta_j}{sd(\\hat \\beta_j)} \\sim N(0,1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**t-statistic**\n",
    "$$t = \\frac{estimate - hypothesized \\ value}{standard \\ error} = \\frac{\\hat \\beta_j - \\beta_j}{se(\\hat \\beta_j)}$$ \n",
    "and has $n-k-1$ degrees of freedom\n",
    "\n",
    "For testing the null hypothesis $H_0: \\beta_j = 0$\n",
    "$$t=\\frac{\\hat \\beta_j}{se(\\hat \\beta_j)}$$\n",
    "\n",
    "To test if the parameters are different,\n",
    "$$t = \\frac{\\hat \\beta_1 - \\hat \\beta_2}{se(\\hat \\beta_1 -\\hat \\beta_2)}$$\n",
    "where $$se(\\hat \\beta_1 -\\hat \\beta_2) = \\sqrt{\\hat {Var} (\\hat \\beta_1 -\\hat \\beta_2)} = \\sqrt {\\hat {Var} (\\hat \\beta_1) + \\hat {Var} (\\hat \\beta_2) - 2\\hat{Cov}(\\hat \\beta_1, \\hat \\beta_2)}$$\n",
    "$$= \\sqrt {[se (\\hat \\beta_1)]^2 + [se (\\hat \\beta_2)]^2 - 2\\hat{Cov}(\\hat \\beta_1, \\hat \\beta_2)}$$\n",
    "This can be solved by setting $\\theta_1 = \\beta_1 - \\beta_2$\n",
    "\n",
    "Standard errors must always be positive because they are estimates of standard deviations.\n",
    "\n",
    "**Confidence Interval**\n",
    "$$\\hat \\beta_j \\pm c \\cdot se(\\hat \\beta_j)$$ where c is the critical value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F-Statistic**\n",
    "$$F = \\frac{(SSR_r - SSR_{ur})/q}{SSR_{ur}/(n-k-1)} \\sim F_{q, n-k-1}$$\n",
    "where $k$ is the number of independent variables in the unrestricted model and $q$ is the number of independent variables removed from the restricted model\n",
    "\n",
    "F tests are valid only under homoskadasticity\n",
    "\n",
    "**Omnibus Test - Overall Significance of the Regression**\n",
    "$$F = \\frac{(SSR_r - SSR_{ur})/q}{SSR_{ur}/(n-k-1)} \\sim F_{q, n-k-1} = \\frac{R^2/k}{(1 - R^2)/(n-k-1)} \\sim F_{k, n-k-1}$$\n",
    "\n",
    "**R squared form of F statistic**\n",
    "$$F = \\frac{(R_{ur}^2 - R_r^2)/q}{(1 - R_{ur}^2)/(n-k-1)} = \\frac{(R_{ur}^2 - R_r^2)/q}{(1 - R_{ur}^2)/df_{ur}}$$\n",
    "++ This cannot be applied for testing all linear restrictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standard Coefficients (Beta Coefficients)**\n",
    "$$\\hat b_j = (\\hat \\sigma_j / \\hat \\sigma_y)\\hat \\beta_j \\ for \\ j = 1,2,...,k$$\n",
    "where $\\hat \\sigma_j$ is the sample standard deviation for $x_j$ and $\\hat \\sigma_y$ is the sample standard deviation for the dependent variable.\n",
    "\n",
    "The standardized regression model is given by\n",
    "$$(y_i - \\overline y)/\\hat \\sigma_y = (\\hat \\sigma_1/\\hat \\sigma_y)\\hat \\beta_1[(x_{i1} - \\overline x_1)/\\hat \\sigma_1] + ... + (\\hat \\sigma_k/\\hat \\sigma_y)\\hat \\beta_k[(x_{ik} - \\overline x_k)/\\hat \\sigma_k] + (\\hat u_i/\\hat \\sigma_y)$$\n",
    "$$z_y = \\hat b_1z_1 + \\hat b_2z_2 + ... + \\hat b_kz_k + error$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Log Functions**\n",
    "\n",
    "Exact percentage change is given by\n",
    "$$\\text{%}\\Delta \\hat y = 100 \\cdot [exp(\\hat \\beta_2 \\Delta x_2) - 1]$$\n",
    "\n",
    "**Quadratic Functions**\n",
    "\n",
    "If $$\\hat y = \\hat \\beta_0 + \\hat \\beta_1 x + \\hat \\beta_2 x^2$$ then\n",
    "$$\\Delta \\hat y \\approx (\\hat \\beta_1 + 2 \\hat \\beta_2 x) \\Delta x \\implies \\Delta \\hat y/\\Delta x \\approx \\hat \\beta_1 + 2 \\hat \\beta_2 x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Percentage Diff for Dummy Variables**\n",
    "\n",
    "If $\\hat \\beta_1$ is the coefficient on a dummy variable $x_1$, when $log(y)$ is the dependent variable, the exact percentage difference in the predicted $y$ when $x_1 = 1$ versus when $x_1 = 0$ is\n",
    "$$100 \\cdot [exp(\\hat \\beta_1) - 1]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chow Statistic**\n",
    "$$F = \\frac{[SSR_P - (SSR_1 + SSR_2)]}{SSR_1 + SSR_2} \\frac{[n-2(k+1)]}{k+1}$$\n",
    "where $SSR_1 + SSR_2$ is the sum of squared resisuals for the unrestricted model $SSR_{ur}$, $SSR_P$ is the restricted sum of squared residuals"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
